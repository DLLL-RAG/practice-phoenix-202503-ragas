{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc809654-26b2-4972-a10b-835bef0fdf39",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "https://docs.arize.com/phoenix/datasets-and-experiments/use-cases-datasets/summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8194-e7be-4d86-8f69-eb23f15a0092",
   "metadata": {},
   "source": [
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23867f46-af76-4787-9450-97d950e5f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments\n",
    "pd.set_option(\"display.max_colwidth\", None)  # display full cells of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df78b82-1d3e-4cc2-9aa4-e4af9b328d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instrument Your Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d5f8c5-6695-43d1-a63b-29e983fb3fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://phoenix:6006/v1/traces\"\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf988ac5-688a-4330-a712-da3c5a3b0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a267bf-d35f-417c-a641-2a0ce51cac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a70453b96347a0bd52ad1f3ebb6e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec04dd278bf497ab6317220d044cb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3809958672f84753a402e0e7d85ca015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6f0e31d2eb4294a5c5f41b10c4b22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdc2549a91c41bba42038fffaaa5fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e92ce968d147ff8dc2c2615c925eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2af39db782a47339d8cf24457bbfc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95636c8757b451792ff6092772b5274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42669a30a9f4e83bad0fd73e306a706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Uploading dataset...\n",
      "💾 Examples uploaded: http://phoenix:6006/datasets/RGF0YXNldDo0/examples\n",
      "🗄️ Dataset version ID: RGF0YXNldFZlcnNpb246Ng==\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "hf_ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "df = (\n",
    "    hf_ds[\"test\"]\n",
    "    .to_pandas()\n",
    "    .sample(n=10, random_state=0)\n",
    "    .set_index(\"id\")\n",
    "    .rename(columns={\"highlights\": \"summary\"})\n",
    ")\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "dataset = px.Client(endpoint=\"http://phoenix:6006\").upload_dataset(\n",
    "    dataframe=df,\n",
    "    input_keys=[\"article\"],\n",
    "    output_keys=[\"summary\"],\n",
    "    dataset_name=f\"news-article-summaries-{now}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce901d-dc00-4cac-a9cc-b34fe483b455",
   "metadata": {},
   "source": [
    "## Define Your Experiment Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b834f06a-33a0-4e26-a3c0-5a19808cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# from phoenix.experiments import Example\n",
    "import os\n",
    "\n",
    "# Create OpenAI client with custom base URL\n",
    "openai_client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE\")  # Custom API Base\n",
    ")\n",
    "\n",
    "async def summarize_article_openai(example, prompt_template: str, model: str) -> str:\n",
    "    formatted_prompt_template = prompt_template.format(article=example.input[\"article\"])\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"assistant\", \"content\": formatted_prompt_template},\n",
    "        ],\n",
    "    )\n",
    "    assert response.choices\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b128e37-5f70-4b93-a16e-19752ac418e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiger Woods' return to competitive golf at Augusta was marked by inconsistency. Despite his\n",
      "impressive short game, Woods struggled with accuracy off the tee and experienced moments of\n",
      "frustration. While he showed glimpses of his former brilliance with remarkable shots, his overall\n",
      "performance reflected a golfer still battling against age and past injuries.  Woods finished the\n",
      "first round one over par, highlighting both his resilience and the challenges he faces in regaining\n",
      "his dominant form.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from functools import partial\n",
    "\n",
    "template = \"\"\"\n",
    "Summarize the article in two to four sentences:\n",
    "\n",
    "ARTICLE\n",
    "=======\n",
    "{article}\n",
    "\n",
    "SUMMARY\n",
    "=======\n",
    "\"\"\"\n",
    "task = partial(summarize_article_openai, prompt_template=template, model=\"o1\")\n",
    "# print(dataset.examples)\n",
    "test_example = dataset[0]\n",
    "print(textwrap.fill(await task(test_example), width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ed851-dc0c-4ec8-8678-983d3c3ecee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Your Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0876533b-8ca7-40fb-9c0e-95278c70d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "# convenience functions\n",
    "def _rouge_1(hypothesis: str, reference: str) -> Dict[str, Any]:\n",
    "    scores = Rouge().get_scores(hypothesis, reference)\n",
    "    return scores[0][\"rouge-1\"]\n",
    "\n",
    "\n",
    "def _rouge_1_f1_score(hypothesis: str, reference: str) -> float:\n",
    "    return _rouge_1(hypothesis, reference)[\"f\"]\n",
    "\n",
    "\n",
    "def _rouge_1_precision(hypothesis: str, reference: str) -> float:\n",
    "    return _rouge_1(hypothesis, reference)[\"p\"]\n",
    "\n",
    "\n",
    "def _rouge_1_recall(hypothesis: str, reference: str) -> float:\n",
    "    return _rouge_1(hypothesis, reference)[\"r\"]\n",
    "\n",
    "\n",
    "# evaluators\n",
    "def rouge_1_f1_score(output: str, expected: Dict[str, Any]) -> float:\n",
    "    return _rouge_1_f1_score(hypothesis=output, reference=expected[\"summary\"])\n",
    "\n",
    "\n",
    "def rouge_1_precision(output: str, expected: Dict[str, Any]) -> float:\n",
    "    return _rouge_1_precision(hypothesis=output, reference=expected[\"summary\"])\n",
    "\n",
    "\n",
    "def rouge_1_recall(output: str, expected: Dict[str, Any]) -> float:\n",
    "    return _rouge_1_recall(hypothesis=output, reference=expected[\"summary\"])\n",
    "\n",
    "\n",
    "def num_tokens(output: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(output))\n",
    "\n",
    "\n",
    "EVALUATORS = [rouge_1_f1_score, rouge_1_precision, rouge_1_recall, num_tokens]\n",
    "# EVALUATORS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c444a48-8505-4707-b374-37a0df651598",
   "metadata": {},
   "source": [
    "## Run Experiments and Iterate on Your Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f47feee2-9b04-4a26-a2d8-3b57ef571449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Experiment started.\n",
      "📺 View dataset experiments: http://phoenix:6006/datasets/RGF0YXNldDo0/experiments\n",
      "🔗 View this experiment: http://phoenix:6006/datasets/RGF0YXNldDo0/compare?experimentId=RXhwZXJpbWVudDo3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c23c1843664ddf94dd0d2672d74deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/10 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec758acac7242c09d84e1a0e6376351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/40 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 View this experiment: http://phoenix:6006/datasets/RGF0YXNldDo0/compare?experimentId=RXhwZXJpbWVudDo3\n",
      "\n",
      "Experiment Summary (03/10/25 06:50 PM +0000)\n",
      "--------------------------------------------\n",
      "           evaluator   n  n_scores   avg_score\n",
      "0         num_tokens  10        10  108.500000\n",
      "1   rouge_1_f1_score  10        10    0.299874\n",
      "2  rouge_1_precision  10        10    0.254611\n",
      "3     rouge_1_recall  10        10    0.377769\n",
      "\n",
      "Tasks Summary (03/10/25 06:50 PM +0000)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          10      10         0\n"
     ]
    }
   ],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "experiment_results = run_experiment(\n",
    "    dataset,\n",
    "    task,\n",
    "    experiment_name=\"initial-template\",\n",
    "    experiment_description=\"first experiment using a simple prompt template\",\n",
    "    experiment_metadata={\"vendor\": \"openai\", \"model\": \"o1\"},\n",
    "    evaluators=EVALUATORS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cb75e-aa1e-44ec-b86f-dd4673e47554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
