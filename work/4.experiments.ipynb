{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc809654-26b2-4972-a10b-835bef0fdf39",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "https://docs.arize.com/phoenix/datasets-and-experiments/use-cases-datasets/summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e8194-e7be-4d86-8f69-eb23f15a0092",
   "metadata": {},
   "source": [
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23867f46-af76-4787-9450-97d950e5f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments\n",
    "pd.set_option(\"display.max_colwidth\", None)  # display full cells of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df78b82-1d3e-4cc2-9aa4-e4af9b328d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instrument Your Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d5f8c5-6695-43d1-a63b-29e983fb3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://phoenix:6006/v1/traces\"\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf988ac5-688a-4330-a712-da3c5a3b0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a267bf-d35f-417c-a641-2a0ce51cac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14861bdcb0ee4e5ca9dc2e7eb085c778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615fb3e8d08145338daaeeed21b21ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f1eb77ff0e4f43a6dd943f7b3977c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6a01e964e54ae68d77f1f50a6f302b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4851e21356646ea9f502f95bd4a80f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc412f025d54638ace9ac4d7a65150d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883e60b2ba444a6ca676f04c89b04a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9a5a6cdaa3442790e07ef458cf2fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c1de82cfbb48cbaa28ca9fc8c3c1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://phoenix:6006/datasets/RGF0YXNldDoy/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246Mg==\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "hf_ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "df = (\n",
    "    hf_ds[\"test\"]\n",
    "    .to_pandas()\n",
    "    .sample(n=10, random_state=0)\n",
    "    .set_index(\"id\")\n",
    "    .rename(columns={\"highlights\": \"summary\"})\n",
    ")\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "dataset = px.Client(endpoint=\"http://phoenix:6006\").upload_dataset(\n",
    "    dataframe=df,\n",
    "    input_keys=[\"article\"],\n",
    "    output_keys=[\"summary\"],\n",
    "    dataset_name=f\"news-article-summaries-{now}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce901d-dc00-4cac-a9cc-b34fe483b455",
   "metadata": {},
   "source": [
    "## Define Your Experiment Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b834f06a-33a0-4e26-a3c0-5a19808cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# from phoenix.experiments import Example\n",
    "import os\n",
    "\n",
    "# Create OpenAI client with custom base URL\n",
    "openai_client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE\")  # Custom API Base\n",
    ")\n",
    "\n",
    "async def summarize_article_openai(example, prompt_template: str, model: str) -> str:\n",
    "    formatted_prompt_template = prompt_template.format(article=example.input[\"article\"])\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"assistant\", \"content\": formatted_prompt_template},\n",
    "        ],\n",
    "    )\n",
    "    assert response.choices\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b128e37-5f70-4b93-a16e-19752ac418e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiger Woods struggled during the first round at Augusta, showing both flashes of brilliance and\n",
      "moments of frustration. While his short game showed improvement, his erratic driving and occasional\n",
      "mistakes hindered his performance. Despite the challenges, Woods demonstrated his resilience and\n",
      "managed to salvage a respectable score, leaving fans wondering if he can recapture his former glory.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from functools import partial\n",
    "\n",
    "template = \"\"\"\n",
    "Summarize the article in two to four sentences:\n",
    "\n",
    "ARTICLE\n",
    "=======\n",
    "{article}\n",
    "\n",
    "SUMMARY\n",
    "=======\n",
    "\"\"\"\n",
    "task = partial(summarize_article_openai, prompt_template=template, model=\"o1\")\n",
    "# print(dataset.examples)\n",
    "test_example = dataset[0]\n",
    "print(textwrap.fill(await task(test_example), width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ed851-dc0c-4ec8-8678-983d3c3ecee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Your Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0876533b-8ca7-40fb-9c0e-95278c70d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "# convenience functions\n",
    "def _rouge_1(hypothesis: str, reference: str) -> Dict[str, Any]:\n",
    "    scores = Rouge().get_scores(hypothesis, reference)\n",
    "    return scores[0][\"rouge-1\"]\n",
    "\n",
    "\n",
    "def _rouge_1_f1_score(hypothesis: str, reference: str) -> float:\n",
    "    return _rouge_1(hypothesis, reference)[\"f\"]\n",
    "\n",
    "\n",
    "def _rouge_1_precision(hypothesis: str, reference: str) -> float:\n",
    "    return _rouge_1(hypothesis, reference)[\"p\"]\n",
    "\n",
    "\n",
    "def _rouge_1_recall(hypothesis: str, reference: str) -> float:\n",
    "    return _rouge_1(hypothesis, reference)[\"r\"]\n",
    "\n",
    "\n",
    "# evaluators\n",
    "def rouge_1_f1_score(output: str, expected: Dict[str, Any]) -> float:\n",
    "    return _rouge_1_f1_score(hypothesis=output, reference=expected[\"summary\"])\n",
    "\n",
    "\n",
    "def rouge_1_precision(output: str, expected: Dict[str, Any]) -> float:\n",
    "    return _rouge_1_precision(hypothesis=output, reference=expected[\"summary\"])\n",
    "\n",
    "\n",
    "def rouge_1_recall(output: str, expected: Dict[str, Any]) -> float:\n",
    "    return _rouge_1_recall(hypothesis=output, reference=expected[\"summary\"])\n",
    "\n",
    "\n",
    "def num_tokens(output: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(output))\n",
    "\n",
    "\n",
    "EVALUATORS = [rouge_1_f1_score, rouge_1_precision, rouge_1_recall, num_tokens]\n",
    "# EVALUATORS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c444a48-8505-4707-b374-37a0df651598",
   "metadata": {},
   "source": [
    "## Run Experiments and Iterate on Your Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f47feee2-9b04-4a26-a2d8-3b57ef571449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://phoenix:6006/datasets/RGF0YXNldDoy/experiments\n",
      "üîó View this experiment: http://phoenix:6006/datasets/RGF0YXNldDoy/compare?experimentId=RXhwZXJpbWVudDox\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cad893352e047e38c9bf9ef13131cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb4d8273f994e6aad698c4b28db8a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/40 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://phoenix:6006/datasets/RGF0YXNldDoy/compare?experimentId=RXhwZXJpbWVudDox\n",
      "\n",
      "Experiment Summary (03/11/25 07:49 AM +0000)\n",
      "--------------------------------------------\n",
      "           evaluator   n  n_scores  avg_score\n",
      "0         num_tokens  10        10  84.100000\n",
      "1   rouge_1_f1_score  10        10   0.329933\n",
      "2  rouge_1_precision  10        10   0.314574\n",
      "3     rouge_1_recall  10        10   0.358298\n",
      "\n",
      "Tasks Summary (03/11/25 07:49 AM +0000)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          10      10         0\n"
     ]
    }
   ],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "experiment_results = run_experiment(\n",
    "    dataset,\n",
    "    task,\n",
    "    experiment_name=\"initial-template\",\n",
    "    experiment_description=\"first experiment using a simple prompt template\",\n",
    "    experiment_metadata={\"vendor\": \"openai\", \"model\": \"o1\"},\n",
    "    evaluators=EVALUATORS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cb75e-aa1e-44ec-b86f-dd4673e47554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
